### [<u>Deep reinforcement learning: a survey</u>](https://doi.org/10.1631/FITEE.1900533)

focus on different algorithms(algo)

#### Model-free

<u>time consuming, better performance</u>

##### 1 value function

<u>easy to apply, especially in dicrete action space; learn implicit policies, efficient, easy to converge</u>

algo: DQN, DDQN, A3C(advantage AC), Noisy DQN, Rainbow(combine version)

*Dueling Architecture*：Q=V+A (focus: target(future) and opponent/nearby objects(now))

   *Advantage(A)*： algo: GAE 通過引入超參數$\lambda$統一此前計算A嘅算法$(TD(\lambda),MC)$，平衡error同variance

##### 2 policy gradient

<u>avoid policy degradation(value function error), could easily apply in continuous action space, learn stochastice policies, low data efficiency, large variance(hard to converge)</u>

###### 2.1 AC

algo: $\textnormal{DDPG, TD3(Twin delayed DDPG)  (deterministic policy)}\\\textnormal{SAC}$

*SAC(Soft AC)* stochastic AC formulation & maximum entropy framework

###### 2.2 trust region

<u>bad efficiency(demand lots of new samples when update), but update stably</u>

algo: $\textnormal{TRPO, ACER, PPO, A3C, PCL}$

#### Model-based

<u>train much faster, model error and overfitting lead to performing a little worse</u>

##### 1 global and local

###### 1.1 global

<u>cheap computation, bad stability in stochastic domains</u>

###### 1.2 local (more attention is paid to)

algo: iLQR, Dyna-Q, I2As

##### 2 Uncertainty-aware

###### 2.1 estimate mode uncertainty

algo: PILCO, variational Bayesian deep learning, PETS

###### 2.2 using output entropy

##### 3 complex observations(partially observable)

algo: E2C(embed to control), SOLAP

#### Help exploration

##### 1 optimistic ~N(s)

##### 2 posterior sampling

##### 3 Information gain

algo: VIME(variational information maximizing exploration), curiosity-driven(intrinsic reward)

#### Inverse RL

learn raward function

based on maximum margin or maximum entropy

#### Transfer RL

Meta-RL

#### Challenges and future

##### inefficient sample

##### demanding good reward function

##### overfitting and instability



1.可以再睇得快啲，唔好一行行噉睇

2.大概瀏覽下文章結構同邏輯，再逐個去睇

3.有唔知唔明嘅可以先mark低，後邊再查資料

4.可以邊做筆記邊睇，唔使話總結嗰時要再睇