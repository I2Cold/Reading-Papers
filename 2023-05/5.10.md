[Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning](https://doi.org/10.48550/arXiv.2206.04384)

w.r.t. with respect to



好得意嘅idea，將狀態動作空間映射到圖嘅結點同邊（似一種離散化方法），簡化濃縮資訊；再用路徑規劃嚟指導策略，非常絕妙

而且實驗結果可以可視化呢點好正，可解釋性強

同埋可以通過重寫reward去學識其他skill，都幾impressive



不足之處都有提到：

1.用喺 offline 設置，如果係online就要考慮點樣動態重建，以及平衡 explore 同 exploit

2.actor translator確實可以用好啲嘅，直接BC好可能只係 suboptimal

3.hyperparameters嘅設置有啲多（圖嘅維度、聚類最大距離、折扣因子，推理步長……），而且係task-specific



我自己嘅話

1.覺得將$P_G(v_i\rightarrow v_j)$直接分爲0或1係咪唔太合理呢？但唔知環境情況又確實唔容易去估計，估計呢個要同$R_G(v_i,v_j)$配合使用先會產生更大效果

2.雖然就噉將距離相近嘅merge喺一齊比較make sense，但肯定都丟失咗信息，相對應嘅optimal action可能就會有唔細嘅偏離。感覺上可以喺構建圖嘅時候做更多文章，使佢有某啲特性（例如同一方向嘅邊嘅動作都相近）maybe會更好？呢個要再具體睇下code點實現先知道work唔work